1. Dels valde jag cURL och DomDocument med Xpath för att det fanns introduktionsvideor till dessa, men samtidigt för att cURL på ett smidigt vis kan ställas in på många olika sätt. Hanteringen och kakor och automatiskt följande av location gjorde en del av mitt arbete lättare.

2. 1. Skulle designen ändras kommer skraparen förmodligen inte att fungera längre. 2. Kan vara en onödig belastning på servern då mycket data måste hämtas hem för att kanske egentligen bara plocka ut lite av den. Bättre med APIer. 3. Väljer den som publicerar datan att ändra på innehållet kan det få elaka följder för den som skrapar innehållet. Felaktigt resultat kan komma att publiceras.

3. Jag har ej blivit klar med ASP.NET WebForms så inte helt hundra på hur detta skiljer sig, men tror att man måste skicka upp mer data till servern för att det ska bli rätt med session och annan persistens om jag inte har helt fel för mig.

4. Vet inte om jag skapat en god webbskrapare direkt. Jag har satt en sleep(1) i koden för att få det att se naturligare ut och minska belastningen (främst för att minska risken att dom ska tro att jag är en skrapare dock och att inte flera anrop görs på en och samma gång). Givetvis vill jag minimera antalet anrop men blir ändå en hel del.

5. Har lärt mig att det kan vara lite bökigt att skrapa sidor med inloggning för kan finnas saker som man måste skicka med som man inte har full koll på. I mitt fall AuthURL. En annan sak är att jag hade lite problem med att cookies inte sparades ner ordentligt så jag skapar en ny cURL i de flesta fall för att "mellanlanda lite kakor". (Tror i alla fall att det var detta som var problemet)